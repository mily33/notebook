# Hyper-parameter optimization

目前主要[方法](https://www.jeremyjordan.me/hyperparameter-tuning/)有:
- 网格搜索
- 随机搜索
- 贝叶斯优化（效果最好）
- 遗传算法

### Reinforcement learning

[Learning to optimize](https://arxiv.org/pdf/1606.01885.pdf) 

[Learning to learn by gradient descent by gradient descent](https://arxiv.org/pdf/1606.04474.pdf)

[Using Deep Q-Learning to Control Optimization Hyperparameters](https://arxiv.org/pdf/1602.04062.pdf)
### Meta-learning

[Optimization as a model for few-shot learning](https://openreview.net/pdf?id=rJY0-Kcll)

[A bridge between hyperparameter optimization and learning-to-learn](http://metalearning.ml/papers/metalearn17_franceschi.pdf)

### Evolution

[Population Based Training of Neural Networks](https://arxiv.org/pdf/1711.09846.pdf)

### 利用meta-learning改进Bayesian optimization

[Scalable Meta-Learning for Bayesian Optimization](https://arxiv.org/pdf/1802.02219.pdf)

[Initializing Bayesian Hyperparameter Optimization via Meta-Learning](file:///home/mily/Downloads/10029-44163-1-PB.pdf)

### 利用贝叶斯优化改进强化学习

[Using Bayesian Optimization for Reinforcement Learning](https://blog.sigopt.com/posts/using-bayesian-optimization-for-reinforcement-learning)
