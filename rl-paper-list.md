## reward shaping / encourage exploration

1. Tang, Haoran, et al. "**# Exploration: A study of count-based exploration for deep reinforcement learning.**" Advances in neural information processing systems. 2017.

2. Knox, W. Bradley, and Peter Stone. "**Combining manual feedback with subsequent MDP reward signals for reinforcement learning**." Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 1-Volume 1. International Foundation for Autonomous Agents and Multiagent Systems, 2010.

3. Achiam, Joshua, and Shankar Sastry. "**Surprise-based intrinsic motivation for deep reinforcement learning**." arXiv preprint arXiv:1703.01732 (2017).

4.  Conti, Edoardo, et al. "**Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents**." Advances in Neural Information Processing Systems. 2018.

5. Fu, Justin, John Co-Reyes, and Sergey Levine. "**Ex2: Exploration with exemplar models for deep reinforcement learning**." Advances in Neural Information Processing Systems. 2017.

6. Houthooft, Rein, et al. "**Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks**." arXiv preprint arXiv:1605.09674 (2016).

## curriculum learning

1. Florensa, Carlos, et al. "**Reverse curriculum generation for reinforcement learning**." arXiv preprint arXiv:1707.05300 (2017).

## Hierachical reinforcement learning

1. Vezhnevets, Alexander Sasha, et al. "**Feudal networks for hierarchical reinforcement learning**." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.

2. Kulkarni, Tejas D., et al. "**Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation**." Advances in neural information processing systems. 2016.

3. Andreas, Jacob, Dan Klein, and Sergey Levine. "**Modular multitask reinforcement learning with policy sketches**." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.




## learning from demonstrations

1. Smart, William D., and L. Pack Kaelbling. "**Effective reinforcement learning for mobile robots**." Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292). Vol. 4. IEEE, 2002.

2. Večerík, Matej, et al. "**Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards**." arXiv preprint arXiv:1707.08817 (2017).

3. Nair, Ashvin, et al. "**Overcoming exploration in reinforcement learning with demonstrations**." 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018.

4. Brys, Tim, et al. "R**einforcement learning from demonstration through shaping**." Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.

5. **Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration** (top 13%) -> solve the sparse rewards problem on web UI tasks using exploration guided by demonstrations.

6. Rajeswaran, Aravind, et al. "**Learning complex dexterous manipulation with deep reinforcement learning and demonstrations**." arXiv preprint arXiv:1709.10087 (2017).

7. Christiano, Paul F., et al. "**Deep reinforcement learning from human preferences**." Advances in Neural Information Processing Systems. 2017.

## pre-train without reward function

1. Florensa, Carlos, Yan Duan, and Pieter Abbeel. "**Stochastic neural networks for hierarchical reinforcement learning**." arXiv preprint arXiv:1704.03012 (2017).

2. Gupta, Abhishek, et al. "**Learning invariant feature spaces to transfer skills with reinforcement learning.**" arXiv preprint arXiv:1703.02949 (2017).





## inverse RL

1. Abbeel, Pieter, and Andrew Y. Ng. "**Apprenticeship learning via inverse reinforcement learning**." Proceedings of the twenty-first international conference on Machine learning. ACM, 2004.


## meta-learning

1. Riedmiller, Martin, et al. "**Learning by playing-solving sparse reward tasks from scratch**." arXiv preprint arXiv:1802.10567 (2018).
2. Frans, Kevin, et al. "**Meta learning shared hierarchies**." arXiv preprint arXiv:1710.09767 (2017).

## algorithm

1. **TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning** (top 9%) -> re-examine the role of TD in modern deep RL, using specially designed environments that each control for a specific factor that affects performance, such as reward sparsity, reward delay or the perceptual complexity of the task.

2. **Residual loss prediction: reinforcement learning with no incremental feedback (ICLR 2018)**
a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.

3. Racanière, Sébastien, et al. "**Imagination-augmented agents for deep reinforcement learning**." Advances in neural information processing systems. 2017. -> combine model-based and model free

4. He, Frank S., et al. "**Learning to play in a day: Faster deep reinforcement learning by optimality tightening**." arXiv preprint arXiv:1611.01606 (2016).
-> combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation.

5. 