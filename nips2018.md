RL Theory
**Non-delusional Q-learning and value-iteration** (best paper) — identify a fundamental source of error in Q-learning and other forms of dynamic programming with function approximation. Delusional bias arises when the approximation architecture limits the class of expressible greedy policies. Introduce a new notion of policy consistency and define a local backup process that ensures global consistency through the use of information sets — -sets that record constraints on policies consistent with backed-up Q-values; suggest other practical heuristics for value-iteration and Q-learning that attempt to reduce delusional bias. Env grid world.

**Multiple-Step Greedy Policies in Approximate and Online Reinforcement Learning** — we study multiple-step greedy algorithms. In the absence of approximations, monotonic policy improvement is not guaranteed unless the update stepsize is sufficiently large.

**Total stochastic gradient algorithms and applications in reinforcement learning** — the total derivative rule leads to an intuitive visual framework for creating gradient estimators on graphical models. Evaluate our methods on model-based policy gradient algorithms, achieve good performance, and present evidence towards demystifying the success of the popular PILCO algorithm.

**Reinforcement Learning with Multiple Experts: A Bayesian Model Combination Approach** — apply Bayesian Model Combination with multiple experts in a way that learns to trust a good combination of experts as training progresses, to improve convergence across discrete and continuous domains and different reinforcement learning algorithms. Env: CartPole and Summary.

**Dual Policy Iteration** — The reactive policy is updated under supervision from the non-reactive policy, while the non-reactive policy is improved with guidance from the reactive policy. Study this Dual Policy Iteration (DPI) strategy in an alternating optimization framework and provide a convergence analysis that extends existing API theory. MuJoCo Env.

**Policy Regret in Repeated Games** — examine policy regret in the game setting; introduce the notion of policy equilibrium and showed that it captures the behavior of no policy regret players.

**Learning convex bounds for linear quadratic control policy synthesis** — use sequential convex programing to learn control policies for unknown linear dynamical systems so as to maximize a quadratic reward function, with experiments on a real-world inverted pendulum.

**Policy-Conditioned Uncertainty Sets for Robust Markov Decision Processes** — propose non-rectangular uncertainty sets that bound marginal moments of state-action features defined over entire trajectories through a decision process. This enables generalization to different portions of the state space while retaining appropriate uncertainty of the decision process. Env grid world.
Is Q-Learning Provably Efficient? — prove that, in an episodic MDP setting, Q-learning with UCB exploration achieves certain regret.
RL Network Architecture
Simple random search of static linear policies is competitive for reinforcement learning — introduce a model-free random search algorithm for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. The evaluations are of high variance.
Genetic-Gated Networks for Deep Reinforcement Learning — Inspired by Dropout, authors introduce the Genetic-Gated Networks (G2Ns), which combines a gate vector composed of binary genetic genes in the hidden layer(s) of networks, to gate feed-forwarding flow of the neural network to create different models with chromosome vectors generated from a genetic algorithm. Improved sampling efficiency and performance on Atari and MuJoCo environments.
RL Algorithms
Negotiable Reinforcement Learning for Pareto Optimal Sequential Decision-Making — Poster — For an agent making decisions on behalf of two or more principals with different priors on the dynamics of the environment, we proposed a Negotiable Reinforcement Learning (NRL) framework: the relative weight given to each principal’s utility should evolve over time according to how well the agent’s observations conform with that principal’s prior, with experiments in a simple grid world environment.
Q-learning with Nearest Neighbors — when only a single sample path under an arbitrary policy of the system is available, consider the Nearest Neighbor Q-Learning (NNQL) algorithm to learn the optimal Q function using nearest neighbor regression method.
Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion — propose stochastic ensemble value expansion (STEVE),dynamically interpolating between model rollouts of various horizon lengths for each individual example, STEVE ensures that the model is only utilized when doing so does not introduce significant errors, which outperforms model-free baselines on MuJoCo environments.
Evolution-Guided Policy Gradient in Reinforcement Learning — Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. Env MuJoCo.
Fast deep reinforcement learning using online adjustments from the past — propose Ephemeral Value Adjustments (EVA) to rapidly adapt to experience in their replay buffer, by shifting the value predicted by a neural network with an estimate of the value function found by planning over experience tuples from the replay buffer near the current state. Env: gridworld and Atari games.
Evolved Policy Gradients —a meta-learning algorithms that evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent’s experience, for faster learning on several randomized environments compared to an off-the-shelf policy gradient method. Env: MuJoCo.
Balanced Policy Evaluation and Learning — We present a new approach to the problems of evaluating and learning personalized decision policies from observational data of past contexts, decisions, and outcomes: a new, balance-based approach that too makes the data look like the new policy but does so directly by finding weights that optimize for balance between the weighted data and the target policy in the given, finite sample, which is equivalent to minimizing worst-case or posterior conditional mean square error.
Confounding-Robust Policy Improvement — a framework for estimating and optimizing for robust policy improvement, which optimizes the minimax regret of a candidate personalized decision policy against a baseline policy. Assessments on synthetic and clinical data demonstrate the benefits of robust policy improvement.
On Oracle-Efficient PAC RL with Rich Observations — We present new provably sample-efficient algorithms for environments with deterministic hidden state dynamics and stochastic rich observations. With stochastic hidden state dynamics, we prove that the only known sample-efficient algorithm, OLIVE, cannot be implemented in the oracle model. We also present several examples that illustrate fundamental challenges of tractable PAC reinforcement learning in such general settings.
Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning — we propose the Action-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL algorithm with an Action Elimination Network (AEN) that eliminates sub-optimal actions. The AEN is trained to predict invalid actions, supervised by an external elimination signal provided by the environment. Simulations demonstrate a considerable speedup and added robustness over vanilla DQN in text-based games with over a thousand discrete actions.
RL Off-policy Learning
Representation Balancing MDPs for Off-policy Policy Evaluation — estimate both the individual policy value and average policy value accurately, using a finite sample generalization error bound for value estimates as an objective to get a balanced representation, tested in cart pole, mountain car and a HIV treatment simulation domain.
Policy Optimization via Importance Sampling (Oral) — model-free, policy search algorithm, POIS, for alternating online and offline optimization, using high-confidence bound for importance sampling estimation and a surrogate objective function, tested on a selection of simple continuous control tasks.
Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation — a new off-policy estimation method that applies IS directly on the stationary state-visitation distributions to avoid the exploding variance issue faced by existing estimators. key is to estimating the density ratio of two stationary distributions, with trajectories sampled from only the behavior distribution.
An Off-policy Policy Gradient Theorem Using Emphatic Weightings — the first off-policy policy gradient theorem and a new actor-critic algorithm — called Actor Critic with Emphatic weightings (ACE) — that approximates the simplified gradients provided by the theorem. demonstrated in a simple counterexample that previous off-policy policy gradient methods — particularly OffPAC and DPG — converge to the wrong solution whereas ACE finds the optimal solution.
RL Safety, Prove, Verifiability
Constrained Cross-Entropy Method for Safe Reinforcement Learning — We propose a constrained cross-entropy-based method that explicitly tracks its performance with respect to constraint satisfaction. We show that the asymptotic behavior of the proposed algorithm can be almost-surely described by that of an ordinary differential equation. Then we give sufficient conditions on the properties of this differential equation to guarantee the convergence of the proposed algorithm. Env: simple navigation task.
A Lyapunov-based Approach to Safe Reinforcement Learning — To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision problems (CMDPs), augmented with constraints on expected cumulative costs. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local, linear constraints. Env: grid-world.
Verifiable Reinforcement Learning via Policy Extraction — use nonparametric verifiable decision tree policies. Propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function. Env Pong and Cart-pole.
RL Exploration, Reward, Goal
Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents — show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Env Atari and MuJoCo.
Diversity-Driven Exploration Strategy for Deep Reinforcement Learning — To tackle exploration, authors present a diversity-driven approach for exploration by simply adding a distance measure to the loss function and propose an adaptive scaling method for stabilizing the learning process. Env: Atari and MuJoCo.
Exploration in Structured Reinforcement Learning — (Oral) Authors address reinforcement learning problems with finite state and action spaces where the underlying MDP has some known structure that could be potentially exploited to minimize the exploration rates of suboptimal (state, action) pairs. Authors devise DEL (Directed Exploration Learning), an algorithm that matches our regret lower bounds.
Scalable Coordinated Exploration in Concurrent Reinforcement Learning — authors consider a team of reinforcement learning agents that concurrently operate in a common environment, and we develop an approach to efficient coordinated exploration that builds on seed sampling and randomized value function learning. For a higher-dimensional env (Cartpole Swing-Up), the approach learns quickly with far fewer agents than alternative exploration schemes.
The Importance of Sampling in Meta-Reinforcement Learning — interpreting meta-reinforcement learning as the problem of learning how to quickly find a good sampling distribution in a new environment, authors propose two new meta-reinforcement learning algorithms: E-MAML and E-RL^2. Env: high-dim grid-world and maze.
Visual Reinforcement Learning with Imagined Goals — learning latent variable representation of images as state and goal using VAE allows sampling goals in the latent space and computing reward for the reinforcement learning. Env: real robotic arm.
On Learning Intrinsic Rewards for Policy Gradient Methods — use the Optimal Rewards Framework of Singh et al. that defines the optimal intrinsic reward function. Env Atari and MuJoCo.
Model-based RL
Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models — by employing uncertainty-aware dynamics models, we propose probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples. Env: MoJoCo.
Unsupervised Video Object Segmentation for Deep Reinforcement Learning — propose Motion-Oriented REinforcement Learning (MOREL) that learns representation by detecting and segmenting moving objects in unsupervised way by exploiting structure from motion (optical flow) and uses the relevant information for action selection. The agent identifies which objects are critical for decision making and gradually builds a policy based on relevant moving objects. Env: Atari.
Deep State Space Models for Time Series Forecasting — By parametrizing a per-time-series linear state space model with a jointly-learned recurrent neural network, our method retains desired properties of state space models such as data efficiency and interpretability, while making use of the ability to learn complex patterns from raw data offered by deep learning approaches.
Randomized Prior Functions for Deep Reinforcement Learning — To deal with uncertainty, addition of a randomized untrainable prior network to each ensemble member. We prove that this approach is efficient with linear representations, provide simple illustrations of its efficacy with nonlinear representations and show that this approach scales to large-scale problems far better than previous attempts
Deep Reinforcement Learning of Marked Temporal Point Processes — in asynchronous setting, both the actions taken by an agent and the feedback it receives from the environment are asynchronous stochastic discrete events characterized using marked temporal point processes. We define the agent’s policy using the intensity and mark distribution of the corresponding process and then derive a flexible policy gradient method, which embeds the agent’s actions and the feedback it receives into real-valued vectors using deep recurrent neural networks. We apply our methodology to two different applications in personalized teaching and viral marketing and, using data gathered from Duolingo and Twitter, we show that it may be able to find interventions to help learners and marketers achieve their goals more effectively than alternatives.
Recurrent World Models Facilitate Policy Evolution— (Oral) A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model’s extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments.
Single-Agent Policy Tree Search With Guarantees — We introduced two novel tree search algorithms for single-agent problems that are guided by a policy: LevinTS and LubyTS. Both algorithms have guarantees on the number of nodes that they expand before reaching a solution. Env Sokoban.
Deep Generative Markov State Models — We propose a deep generative Markov State Model (DeepGenMSM) learning framework for inference of metastable dynamical systems and prediction of trajectories. After unsupervised training on time series data, the model contains (i) a probabilistic encoder that maps from high-dimensional configuration space to a small-sized vector indicating the membership to metastable (long-lived) states, (ii) a Markov chain that governs the transitions between metastable states and facilitates analysis of the long-time dynamics, and (iii) a generative part that samples the conditional distribution of configurations in the next time step.
Inverse RL
Teaching Inverse Reinforcement Learners via Features and Demonstrations — Authors introduce the teaching risk that measures the potential suboptimality of policies that look optimal to the learner and show that bounds on the teaching risk guarantee that the learner is able to find a near-optimal policy using standard algorithms based on inverse reinforcement learning. Based on these findings, authors suggest a teaching scheme in which the expert can decrease the teaching risk by updating the learner’s worldview, and thus ultimately enable her to find a near-optimal policy. env: grid world.
Occam’s razor is insufficient to infer the preferences of irrational agents — This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam’s razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret.
Hierarchical RL
Data-Efficient Hierarchical Reinforcement Learning — We introduce HIRO: off-policy model-free RL to learn both higher- and lower-level policies efficiently, where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. Our experiments show learned complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations.
Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies — to generalize to a previously-unseen environment characterized by a subtask graph which describes a set of subtasks and their dependencies, authors propose a neural subtask graph solver (NSGS) which encodes the subtask graph using a recursive neural network embedding. Env: two 2D visual domains.
Multi-Agent
Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual Optimization — we propose a double averaging primal-dual optimization algorithm, where each agent iteratively performs averaging over both space and time to incorporate neighboring gradient information and local reward information, respectively, which converges to the optimal solution at a global geometric rate. Env: mountaincar.
Credit Assignment For Collective Multiagent RL With Global Rewards — for the problem of multiagent credit assignment, authors develop collective actor-critic RL approaches for a general setting where system reward is not decomposable among agents. Env: Real world Taxi Supply-Demand Matching and Police Patrolling problems.
Multi-Agent Generative Adversarial Imitation Learning — multi-agent imitation learning is hard due to multiple (Nash) equilibria and non-stationary environments. We build a multi-agent actor-critic algorithm for general Markov games.Env: Multi-agent particle environment.
Learning Attentional Communication for Multi-Agent Cooperation — When there is a large number of agents, agents cannot differentiate valuable information that helps cooperative decision making from globally shared information. Authors propose an attentional communication model that learns when communication is needed and how to integrate shared information for cooperative decision making. Env: Multi-agent particle environment.
Online Robust Policy Learning in the Presence of Unknown Adversaries — Meta-Learned Advantage Hierarchy (MLAH) framework that is attack model-agnostic and more suited to reinforcement learning, via handling the attacks in the decision space (as opposed to data space) and directly mitigating learned bias introduced by the adversary. Key: learn separate sub-policies (nominal and adversarial) in an online manner, as guided by a supervisory master agent that detects the presence of the adversary by leveraging the advantage function for the sub-policies. Env: MuJoCo.
A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents — deep BPR+ algorithm for efficient policy detecting and reusing techniques when playing against non-stationary agents in Markov games. Env: grid world game.
Actor-Critic Policy Optimization in Partially Observable Multiagent Environments — discuss several update rules for actor-critic algorithms in multiagent reinforcement, with experiments showing that these actor-critic algorithms converge to approximate Nash equilibria in commonly-used benchmark Poker domains with rates similar to or better than baseline model-free algorithms for zero-sum games. The current policy of some variants do significantly better than the baselines (including the average policy of NFSP) when evaluated against fixed bots. Of the actor-critic variants, RPG and QPG seem to outperform RMPG in our experiments. Env: Kuhn poker, Leduc poker.
Learning to Play With Intrinsically-Motivated, Self-Aware Agents — goal is to use curiosity-driven intrinsic motivation to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals; propose a “world-model” network that learns to predict the dynamic consequences of the agent’s actions, and a “self-model” that allows the agent to track the error map of its world-model to adversarially challenge the developing world-model. Env: 3D Unity Simulation.
Learning Others’ Intentional Models in Multi-Agent Settings Using Interactive POMDPs — In order to predict other agents’ actions using Interactive partially observable Markov decision processes, we propose an approach that effectively uses Bayesian inference and sequential Monte Carlo sampling to learn others’ intentional models which ascribe to them beliefs, preferences and rationality in action selection. Empirical results show that our algorithm accurately learns models of the other agent and has superior performance than methods that use subintentional models. Env: multi-agent tiger and UAV reconnaissance.
Meta-Learning, Multi-task, Transfer, Lifelong Learning
Meta-Reinforcement Learning of Structured Exploration Strategies —This work study how prior tasks can be used to inform how exploration should be performed in new tasks. Authors introduce model agnostic exploration with structured noise (MAESN): the prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy. Env: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.
Distributed Multitask Reinforcement Learning with Quadratic Convergence —Multitask reinforcement learning (MTRL) suffers from scalability issues when the number of tasks or trajectories grows large. Recent methods exploited the connection between MTRL and general consensus to propose scalable solutions. We improve over state-of-the-art by deriving multitask reinforcement learning from a variational inference perspective. We then propose a novel distributed solver for MTRL with quadratic convergence guarantees.
Lifelong Inverse Reinforcement Learning —Authors propose the first lifelong learning approach to inverse reinforcement learning, which learns consecutive tasks via demonstration, continually transferring knowledge between tasks to improve performance. Env: object world and highway.
Meta-Gradient Reinforcement Learning — We discuss a gradient-based meta-learning algorithm that is able to adapt the nature of the return, online, whilst interacting and learning from the environment. When applied to 57 games on the Atari 2600 environment over 200 million frames, our algorithm achieved a new state-of-the-art performance.
HOUDINI: Lifelong Learning as Program Synthesis — We present a neurosymbolic framework consists of: (1) a symbolic program synthesizer that performs a type-directed search over parameterized programs, and decides on the library functions to reuse, and the architectures to combine them, while learning a sequence of tasks; and (2) a neural module that trains these programs using stochastic gradient descent. Evaluated on three benchmarks that combine perception with the algorithmic tasks of counting, summing, and shortest-path computation.
Reinforced Continual Learning — Reinforced Continual Learning consists of three networks, controller, value network, and task network, using reinforcement learning to adaptively expand the network. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.
Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning — we introduce a deictic object-oriented representation for reinforcement learning that has provably efficient learning bounds and can solve a broader range of tasks, capable of zero-shot transfer of transition dynamics across tasks. Env: Taxi and Sokoban domains.
Bayesian Model-Agnostic Meta-Learning — we propose a novel Bayesian model-agnostic meta-learning method that combines efficient gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. Experiment results show the accuracy and robustness of the proposed method in sinusoidal regression, image classification, active learning, and reinforcement learning ( MuJoCo).
Probabilistic Model-Agnostic Meta-Learning — propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Experimental results show that the method can sample plausible classifiers and regressors in ambiguous few-shot learning problems.
Applications
Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation — propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which reconciles traditional retrieval-based approaches populated with human prior knowledge, with modern learning-based approaches to achieve structured, robust, and diverse report generation. State-of-the-art results on two medical report datasets.
REFUEL: Exploring Sparse Features in Deep Reinforcement Learning for Fast Disease Diagnosis — propose REFUEL, a reinforcement learning method, to improve the performance of online symptom checking for disease diagnosis. The method can find symptom queries that can yield positive responses from a patient with high probability.
Fighting Boredom in Recommender Systems with Linear Reinforcement Learning — A common assumption in recommender systems (RS) is the existence of a best fixed recommendation strategy. We argue that this assumption is rarely verified in practice, as the recommendation process itself may impact the user’s preferences. We show that a policy considering the long-term influence of the recommendations may outperform both fixed-action and contextual greedy policies in a number of realistic scenarios.
Reinforcement Learning for Solving the Vehicle Routing Problem — solving the Vehicle Routing Problem (VRP) using reinforcement learning, our approach outperforms classical heuristics and Google’s OR-Tools on medium-sized instances in solution quality with comparable computation time (after training). We demonstrate how our approach can handle problems with split delivery and explore the effect of such deliveries on the solution quality.
Inference Aided Reinforcement Learning for Incentive Mechanism Design in Crowdsourcing — For incentive mechanisms for crowdsourcing, we propose a novel inference aided reinforcement mechanism that learns to incentivize high-quality data sequentially and requires no such prior assumptions. We propose a reinforcement incentive learning (RIL) method that dynamically determines the payment without accessing any ground-truth labels.Empirical results show that our mechanism performs consistently well under both rational and non-fully rational (adaptive learning) worker models.
Post: Device Placement with Cross-Entropy Minimization and Proximal Policy Optimization — For device placement in deep learning training, we propose a new joint learning algorithm, Post, that integrates cross-entropy minimization and proximal policy optimization to achieve theoretically guaranteed optimal efficiency. We have implemented Post in the Google Cloud platform, and our extensive experiments with several popular neural network training benchmarks have demonstrated clear evidence of superior performance: with the same amount of learning time, it leads to placements that have training times up to 63.7% shorter over the state-of-the-art.
Visual Memory for Robust Path Following — (Oral) — Given a demonstration of a path, the goal is to re-execute this path either forwards (i.e., following it) or backwards (i.e., homing behavior). To do this, the first network generates an abstraction of the path, and the second network then observes the world and decides how to act in order to retrace the path under noisy actuation and a changing environment. In two realistic simulators, our experiments show that our approach outperforms both a classical approach to solving this task as well as a number of other baselines.
Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing — leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimate. Evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing).
Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation — Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. Propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goal-directed graph generation through reinforcement learning. Large margin improvements on molecular graph generation experiments.
Reinforcement Learning of Theorem Proving — We introduce a theorem proving algorithm that runs Monte-Carlo simulations guided by reinforcement learning from previous proof attempts. The trained system solves within the same number of inferences over 40% more problems than a baseline prover.
Learning Temporal Point Processes via Reinforcement Learning — for ordered event data in continuous time, authors treat the generation of each event as the action taken by a stochastic policy and uncover the reward function using an inverse reinforcement learning. They then derive an efficient policy gradient algorithm for learning flexible point process models, which performs well in both synthetic and real data.
Conclusion
Deep Reinforcement Learning is still one the of biggest and hottest topics.
In addition to training algorithms and theory, meta-learning, multi-agent exploration, and model-based RL are most popular sub-directions.
More and more interesting applications are emerging as well.
Enjoy !

Oh one more thing… links of best papers for your convenience.

Best Papers
NeurIPS2018 Best Paper Awards： 
－ Non-delusional Q-learning and value iteration
－ Optimal Algorithms for Non-Smooth Distributed Optimization in Networks
－ Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes
－ Neural Ordinary Differential Equations

NeurIPS2018 Test of Time Awards： 
－The Tradeoffs of Large-Scale Learning