RL Theory
RESIDUAL LOSS PREDICTION: REINFORCEMENT LEARNING WITH NO INCREMENTAL FEEDBACK (top 22%) -> a novel algorithm for solving reinforcement learning and bandit structured prediction problems with very sparse loss feedback.
Learning Parametric Closed-Loop Policies for Markov Potential Games (top 23%) -> general closed loop analysis for Markov potential games and show that deep reinforcement learning can be used for learning approximate closed-loop Nash equilibrium.
Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning (top 26%) -> an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt.
Reinforcement Learning Algorithm Selection (top 31%) -> online algorithm selection in the context of Reinforcement Learning.
Divide-and-Conquer Reinforcement Learning (top 33%) -> optimizes an ensemble of policies, each on a different slice of the state space, and gradually unifies them into a single policy that can succeed on the whole state space.
RL Algorithms
Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines (oral) (top 12%) -> Action-dependent baselines can be bias-free and yield greater variance reduction than state-only dependent baselines for policy gradient methods.
The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning (top 6%) -> Reactor combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN while giving better run-time performance than A3C.
Action-dependent Control Variates for Policy Optimization via Stein Identity (8%) -> a control variate method to effectively reduce variance for policy gradient methods, motivated by the Steinâ€™s identity.
TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning (top 9%) -> re-examine the role of TD in modern deep RL, using specially designed environments that each control for a specific factor that affects performance, such as reward sparsity, reward delay or the perceptual complexity of the task.
Boosting the Actor with Dual Critic (top 32%) -> from the Lagrangian dual form of the Bellman optimality equation. The algorithm achieves the state-of-the-art performances across several benchmarks.
Temporal Difference Models: Model-Free Deep RL for Model-Based Control (top 35%) -> a special goal-condition value function trained with model free methods can be used within model-based control, resulting in substantially better sample efficiency and performance.
Guide Actor-Critic for Continuous Control (top 45%) -> propose a novel actor-critic method that uses Hessians of a critic to update an actor.
Trust-PCL: An Off-Policy Trust Region Method for Continuous Control (top 52%)-> extend recent insights related to softmax consistency to achieve state-of-the-art results in continuous control.
TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning (top 40%) -> We present TreeQN and ATreeC, new architectures for deep reinforcement learning in discrete-action domains that integrate differentiable on-line tree planning into the action-value function or policy.
RL Network Architecture
Emergence of grid-like representations by training recurrent neural networks to perform spatial localization (top 1%) -> how neural representations of space, including grid-like cells and border cells as observed in the brain, could emerge from training a recurrent neural network to perform navigation tasks.
Neural Map: Structured Memory for Deep Reinforcement Learning (top 5%) -> use the successor representation to discover eigenoptions in stochastic domains, from raw pixels. Eigenoptions are options learned to navigate the latent dimensions of a learned representation.
Active Neural Localization (top 10%) -> a fully differentiable neural network that learns to localize efficiently using deep reinforcement learning.
NerveNet: Learning Structured Policy with Graph Neural Networks (top 12%) -> using graph neural network to model structural information of the agents to improve policy and transferability.
Semi-parametric topological memory for navigation (top 43%) -> a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals.
RL Optimization
Backpropagation through the Void: Optimizing control variates for black-box gradient estimation (top 8%) -> present a general method for unbiased estimation of gradients of black-box functions of random variables. We apply this method to discrete variational inference and reinforcement learning.
Model-Ensemble Trust-Region Policy Optimization (top 16%) -> show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process, and use of likelihood ratio derivatives yields much more stable learning.
Maximum a Posteriori Policy Optimisation (top 33%) -> based on coordinate ascent on a relative-entropy objective. for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.
Policy Optimization by Genetic Distillation (45%) -> present Genetic Policy Optimization (GPO) that uses imitation learning for policy crossover in the state space and applies policy gradient methods for mutation.
RL Exploration
Parameter Space Noise for Exploration (top 17%) -> Combining parameter noise with traditional RL benefits off- and on-policy methods.
DORA The Explorer: Directed Outreaching Reinforcement Action-Selection Memory Augmented Control Networks (top 20%) -> a generalization of visit-counters that evaluate the propagating exploratory value over trajectories, enabling efficient exploration for model-free RL.
Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling (top 27%) -> An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling.
Noisy Networks For Exploration (top 27%) -> A deep reinforcement learning agent with parametric noise added to its weights can be used to aid efficient exploration.
RL Reward
Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play (top 11%) -> Unsupervised learning for reinforcement learning using an automatic curriculum of self-play.
Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration (top 19%) -> We propose a novel Intrinsically Motivated Goal Exploration architecture with unsupervised learning of a space where goals can be sampled, and compare systematically various representation learning algorithms in this context.
Emergent Complexity via Multi-Agent Competition (top 23%) -> competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself.
Learning Robust Rewards with Adverserial Inverse Reinforcement Learning (top 25%) -> propose an adversarial inverse reinforcement learning algorithm capable of learning reward functions which can transfer to new, unseen environments.
Truncated Horizon Policy Search: Combining Reinforcement Learning & Imitation Learning (top 53%) -> theoretically-motivated method combines imitation and reinforcement learning via the idea of reward shaping using an oracle. The IL part is in the form of an oracle that returns a value function, which is an approximation of the optimal value function.
Distributed RL
Distributed Distributional Deterministic Policy Gradients (top 14%) -> We develop an agent that we call the Distributional Deterministic Deep Policy Gradient algorithm, which achieves state of the art performance on a number of challenging continuous control problems.
Distributed Prioritized Experience Replay (top 5%) -> A distributed architecture for deep reinforcement learning at scale, using parallel data-generation to improve the state of the art on the Arcade Learning Environment benchmark in a fraction of the wall-clock training time of previous approaches.
Hierarchical RL
Meta Leanring Shared Hierarchies (top 39%) -> learn hierarchal sub-policies through end-to-end training over a distribution of tasks.
Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning (top 31%) -> a novel framework for efficient multi-task reinforcement learning. Our framework trains agents to employ hierarchical policies that decide when to use a previously learned policy and when to learn a new skill.
Multi-Agent
Learning Deep Mean Field Games for Modeling Large Population Behavior (oral) (top 1%) -> Inference of a mean field game (MFG) model of large population behavior via a synthesis of MFG and Markov decision processes.
Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input (oral) (top 8%) -> A controlled study of the role of environments with respect to properties in emergent communication protocols.
Emergent Translation in Multi-Agent Communication (top 16%) -> a communication game where two agents, native speakers of their own respective languages, jointly learn to solve a visual referential task. We find that the ability to understand and translate a foreign language emerges as a means to achieve shared goals.
Emergent Communication through Negotiation (top 35%) -> We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.
RL Meta-learning, Transfer, Continuing Learning
Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments (oral) (top 1%) -> develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios.
Zero-Shot Visual Imitation (oral) (top 2%) -> Agents can learn to imitate solely visual demonstrations (without actions) at test time after learning from their own experience without any form of supervision at training time.
Modular Continual Learning in a Unified Visual Environment (top 5%) -> a neural module approach to continual learning using a unified visual environment with a large action space.
Learning an Embedding Space for Transferable Robot Skills (top 7%) -> a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space, taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients.
Learning to Multi-Task by Active Sampling (top 21%) -> Letting a meta-learner decide the task to train on for an agent in a multi-task setting improves multi-tasking ability substantially
A Simple Neural Attentive Meta-Learner (top 22%) -> a simple RNN-based meta-learner that achieves SOTA performance on popular benchmarks.
Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control (top 23%) -> A continual learning method that uses distillation to combine expert policies and transfer learning to accelerate learning new skills.
Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm (top 24%) -> Deep representations combined with gradient descent can approximate any learning algorithm.
RL Applications
Ask the Right Questions: Active Question Reformulation with Reinforcement Learning (oral) (top 7%) -> propose an agent that sits between the user and a black box question-answering system and which learns to reformulate questions to elicit the best possible answers.
Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration (top 13%) -> solve the sparse rewards problem on web UI tasks using exploration guided by demonstrations.
Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning ( top 27%) -> learns to walk on a knowledge graph and answer queries.
Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis (top 27%) -> Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.
N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning (top 28%) -> A novel reinforcement learning based approach to compress deep neural networks with knowledge distillation.
Conclusion
Deep Reinforcement Learning is one the of biggest and hottest topics.
In addition to theory and algorithms, meta-learning, continual learning, credit assignment (reward), exploration, hierarchical, multi-agent and network architectures are popular sub-directions for RL.
There a big less-explored space for RL on network architectures, considering the amount of papers on network architectures for vision problems.